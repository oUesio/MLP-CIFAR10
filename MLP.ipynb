{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpLdIQ325p61EJHv+535P4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oUesio/MLP-CIFAR10/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from itertools import product\n",
        "import copy\n"
      ],
      "metadata": {
        "id": "_lUjofyA1hNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1cf853-4a01-4ffb-a439-58095b1cd95f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def image_to_feature_vector(img, p):\n",
        "\n",
        "    # Handle pytorch tensors\n",
        "    img = img.transpose(1, 2, 0)\n",
        "\n",
        "    # Height, Width, Channels\n",
        "    H, W, C = img.shape\n",
        "\n",
        "    # Break image into grid of patches\n",
        "    patches = img.reshape(H // p, p, W // p, p, C)\n",
        "    # Reorder so patches in order\n",
        "    patches = patches.transpose(0, 2, 1, 3, 4)\n",
        "    # Flatten into 1D vector\n",
        "    return patches.reshape(-1)\n",
        "\n",
        "\n",
        "def dataset_to_loader(dataset, batch_size, patch_size, shuffle):\n",
        "    X = [] # Feature vectors\n",
        "    y = [] # Labels\n",
        "    for img, label in dataset:\n",
        "        img = np.array(img)\n",
        "        fv = image_to_feature_vector(img, patch_size)  # 4 or 8\n",
        "        X.append(fv)\n",
        "        y.append(label)\n",
        "\n",
        "    X = np.stack(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    # Create a dataset and dataloader\n",
        "    tensor_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "CT5rNXiO1PNP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Function to check if dictionary exists\n",
        "def dict_exists_in_csv(data, csv_file):\n",
        "    if not os.path.isfile(csv_file):\n",
        "        return False\n",
        "\n",
        "    with open(csv_file, mode='r', newline='') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            match = True\n",
        "            for key, value in data.items():\n",
        "                if key in ['val_acc', 'val_loss']:\n",
        "                    continue\n",
        "                # Convert lists to string for comparison\n",
        "                if isinstance(value, list):\n",
        "                    value = str(value)\n",
        "                if row[key] != str(value):\n",
        "                    match = False\n",
        "                    break\n",
        "            if match:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Write to CSV if not already present\n",
        "def append_dict_to_csv(data, csv_file):\n",
        "    file_exists = os.path.isfile(csv_file)\n",
        "    if not dict_exists_in_csv(data, csv_file):\n",
        "        with open(csv_file, mode='a', newline='') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=data.keys())\n",
        "            if not file_exists:\n",
        "                writer.writeheader()\n",
        "            # Convert lists to strings to store in CSV\n",
        "            row_to_write = {k: str(v) if isinstance(v, list) else v for k, v in data.items()}\n",
        "            writer.writerow(row_to_write)\n",
        "        print(\"Dictionary added to CSV.\")\n",
        "    else:\n",
        "        print(\"Dictionary already exists in CSV.\")\n",
        "\n",
        "def load_params_from_csv(csv_file):\n",
        "    with open(csv_file, \"r\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        row = next(reader)  # exactly one row\n",
        "\n",
        "    hidden_sizes = [int(x) for x in row[\"hidden_sizes\"].split(\"-\")]\n",
        "\n",
        "    # Activation mapping\n",
        "    activation_map = {\n",
        "        \"ReLU\": nn.ReLU,\n",
        "        \"LeakyReLU\": nn.LeakyReLU,\n",
        "        \"GELU\": nn.GELU\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        \"patch_size\": int(row[\"patch_size\"]),\n",
        "        \"optimizer\": row[\"optimizer\"].lower(),\n",
        "        \"learning_rate\": float(row[\"learning_rate\"]),\n",
        "        \"batch_size\": int(row[\"batch_size\"]),\n",
        "        \"hidden_sizes\": hidden_sizes,\n",
        "        \"dropout_rate\": float(row[\"dropout_rate\"]),\n",
        "        \"activation\": activation_map[row[\"activation\"]],\n",
        "        \"weight_decay\": float(row[\"weight_decay\"]),\n",
        "        \"epochs\": int(row[\"epochs\"]),\n",
        "        \"batch_norm\": row[\"batch_norm\"] == \"True\",\n",
        "        \"lr_scheduler\": row[\"learning_rate_scheduler\"]\n",
        "    }\n",
        "\n",
        "    return params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Jco0FVlw9DRQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Model"
      ],
      "metadata": {
        "id": "fywMbjIuAn4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_sizes, activation, dropout_rate, use_batchnorm):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last_size = 3072 # input size for flattened images\n",
        "        # Hidden layers\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(last_size, h))\n",
        "            #if use_batchnorm:\n",
        "            layers.append(nn.BatchNorm1d(h))\n",
        "            layers.append(activation())\n",
        "            if dropout_rate > 0:\n",
        "                layers.append(nn.Dropout(dropout_rate))\n",
        "            last_size = h\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(last_size, 10)) # 10 classes\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "hrpKBfO6qo03"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation"
      ],
      "metadata": {
        "id": "vtxKsyEWA7PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mlp(train_split, val_split, hidden_sizes, activation, dropout_rate, optimizer_name, learning_rate, batch_size, epochs, patience, patch_size, weight_decay, use_batchnorm, learning_rate_scheduler):\n",
        "    # Loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    '''# Split training dataset into 80:20 train/validation\n",
        "    val_size = int(0.1 * len(train_dataset))\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_split, val_split = torch.utils.data.random_split(train_dataset, [train_size, val_size])'''\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = dataset_to_loader(train_split, batch_size, patch_size, shuffle=True)\n",
        "    val_loader = dataset_to_loader(val_split, batch_size, patch_size, shuffle=False)\n",
        "\n",
        "    model = MLP(hidden_sizes, activation, dropout_rate, use_batchnorm)\n",
        "\n",
        "    # Optimiser with weight decay\n",
        "    if optimizer_name == 'adam': optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'sgd': optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'adamw': optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'rmsprop': optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'adagrad': optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'adadelta': optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    elif optimizer_name == 'nadam': optimizer = optim.NAdam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    if learning_rate_scheduler == 'reduce':  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "    elif learning_rate_scheduler == 'step': scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "    elif learning_rate_scheduler == 'expon': scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "    elif learning_rate_scheduler == 'cosine': scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "\n",
        "    # Early stopping\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for X, y in train_loader:\n",
        "            # Normalise inputs [0,1]\n",
        "            #X = X.float() / 255.0\n",
        "            # Clear previous gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass (predictions)\n",
        "            outputs = model(X)\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, y)\n",
        "            # Compute gradients (backpropagation)\n",
        "            loss.backward()\n",
        "            # Update model weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        # No gradients needed for validation\n",
        "        with torch.no_grad():\n",
        "            for val_X, val_y in val_loader:\n",
        "                #val_X = val_X.float() / 255.0\n",
        "                outputs = model(val_X)\n",
        "                loss = criterion(outputs, val_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct += (preds == val_y).sum().item()\n",
        "                total += val_y.size(0)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        # Learning rate adjustement\n",
        "        if learning_rate_scheduler == 'reduce':\n",
        "            scheduler.step(val_loss)\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_acc, val_acc, best_val_loss"
      ],
      "metadata": {
        "id": "uZqtY-J1P7dK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "jefSsMXMtj0W",
        "outputId": "5c0c3155-d972-4a4c-cb3f-310baeafaa4c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\ntrain_transforms = transforms.Compose([\\n    transforms.RandomHorizontalFlip(),\\n    transforms.RandomCrop(32, padding=4),\\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465),\\n                         (0.2023, 0.1994, 0.2010))\\n])\\n\\nval_transforms = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.4914, 0.4822, 0.4465),\\n                         (0.2023, 0.1994, 0.2010))\\n])\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''train_transform = transforms.Compose([\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "])'''\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_aug = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "train_noaug = datasets.CIFAR10(root='./data', train=True, download=True, transform=val_transform)\n",
        "\n",
        "val_size = int(0.1 * len(train_aug))\n",
        "train_size = len(train_aug) - val_size\n",
        "\n",
        "train_indices, val_indices = torch.utils.data.random_split(range(len(train_aug)), [train_size, val_size])\n",
        "\n",
        "train_split = torch.utils.data.Subset(train_aug, train_indices)\n",
        "val_split = torch.utils.data.Subset(train_noaug, val_indices)\n",
        "\n",
        "\n",
        "# Model parameters\n",
        "epochs = 5 # keep as 5 until get better results\n",
        "patience = 5\n",
        "\n",
        "# Hyperparameters\n",
        "patch_sizes = [8]#, 8] # 4\n",
        "optimizers = ['adamw', 'adam'] #, 'rmsprop', 'adagrad', 'adadelta', 'nadam'] # adamW # 'adam' sgd\n",
        "learning_rates = [0.001]#, 0.01]#, 0.002] #, 0.001]#, 0.002] # 0.001\n",
        "batch_sizes = [16, 32]#, 64] #, 64] # 32 64, 128, 256 16\n",
        "hidden_sizes_options = [[1024, 512, 256, 256, 128, 64], [1024, 512, 256, 256, 128, 128, 64]] # [[512, 256, 128], [256, 128]] #[512, 256, 128, 64], [1024, 512, 256, 128, 64], [512, 256, 128], [1024, 512, 256, 256, 128, 128, 64]] [256, 128], [512, 256, 128],\n",
        "dropout_rates = [0.1]#, 0.2] #, 0.2] # 0, 0.05, 0.1,\n",
        "activations = [nn.GELU, nn.LeakyReLU, nn.ReLU] # LeakyReLU # nn.ReLU\n",
        "weight_decays = [0.0005]#, 0.001]#, 0.0005, 0.001] # 0.00001, 0\n",
        "batch_norm = [True, False]#, False] # False\n",
        "learning_rate_schedulers = ['expon', 'cosine', 'reduce'] # 'step'\n",
        "\n",
        "# RIGHT NOW JUST WANT TO TEST LEARNING RATE SCHEDULERS\n",
        "\n",
        "\n",
        "grid = product(patch_sizes, optimizers, learning_rates, batch_sizes, hidden_sizes_options, dropout_rates, activations, weight_decays, batch_norm, learning_rate_schedulers)\n",
        "\n",
        "results = []\n",
        "csv_file = \"/content/drive/MyDrive/data/resultsQ5.csv\"\n",
        "csv_best_params = \"/content/drive/MyDrive/data/Q5_best_params.csv\"\n",
        "\n",
        "if os.path.exists(csv_best_params):\n",
        "    best_val_loss = pd.read_csv(csv_best_params).loc[0, \"val_loss\"]\n",
        "else:\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "for pat, opt, lr, bs, hs, dr, act, wd, bn, lrs in grid:\n",
        "    params = {\"patch_size\": pat, \"optimizer\": opt, \"learning_rate\": lr, \"batch_size\": bs, \"hidden_sizes\": hs, \"dropout_rate\": dr, \"activation\": act.__name__, \"weight_decay\": wd, \"epochs\": epochs, \"batch_norm\": bn, \"learning_rate_scheduler\": lrs}\n",
        "    if not dict_exists_in_csv(params, csv_file):\n",
        "        model, params[\"train_acc\"], params[\"val_acc\"], params[\"val_loss\"] = train_mlp(train_split, val_split, hs, act, dr, opt, lr, bs, epochs, patience, pat, wd, bn, lrs)\n",
        "        append_dict_to_csv(params, csv_file)\n",
        "        results.append(params)\n",
        "        if params[\"val_loss\"] < best_val_loss:\n",
        "            print(f\"!!! Found new best: {params[\"val_loss\"]}\")\n",
        "            best_val_loss = params[\"val_loss\"]\n",
        "            params[\"hidden_sizes\"] = '-'.join(map(str, params[\"hidden_sizes\"]))\n",
        "            pd.DataFrame(params, index=[0]).to_csv(csv_best_params, index=False) # Replace if found better\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/data/Q5_mlp.pt\")\n",
        "        print(f\"PATCH: {pat}, OPT: {opt}, LR: {lr}, BS: {bs}, HS: {hs}, DR: {dr}, ACT: {act.__name__}, WD: {wd}, BN: {bn}, LRS: {lrs}, EPOCHS: {epochs}, Train Acc: {params[\"train_acc\"]:.4f}, Val Acc: {params[\"val_acc\"]:.4f}, Val Loss: {params[\"val_loss\"]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flcIyA0qAiNH",
        "outputId": "193d0e24-b5c7-45ac-db68-199a2b909f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary added to CSV.\n",
            "PATCH: 8, OPT: adamw, LR: 0.001, BS: 16, HS: [1024, 512, 256, 256, 128, 64], DR: 0.1, ACT: GELU, WD: 0.0005, BN: True, LRS: expon, EPOCHS: 5, Train Acc: 0.4037, Val Acc: 0.4504, Val Loss: 1.5136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = \"/content/drive/MyDrive/data/resultsQ5.csv\"\n",
        "\n",
        "results_df = pd.read_csv(csv_file)\n",
        "\n",
        "# Sort by validation accuracy\n",
        "#results_df = results_df.sort_values(by=\"val_acc\", ascending=False)\n",
        "results_df = results_df[results_df[\"epochs\"] == 5].sort_values(by=\"val_acc\", ascending=False)\n",
        "\n",
        "# Select top 10 combinations\n",
        "top_combinations = results_df.head(20)\n",
        "\n",
        "# Display top combinations\n",
        "print(top_combinations)\n",
        "\n",
        "# Best parameter combination\n",
        "\n"
      ],
      "metadata": {
        "id": "ogVTPVEJCbpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "FPlUj8_uAfyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_mlp(model_path):\n",
        "    csv_best_params = \"/content/drive/MyDrive/data/Q5_best_params.csv\"\n",
        "    params = load_params_from_csv(csv_best_params)\n",
        "\n",
        "    # Create test loader\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "    test_loader = dataset_to_loader(test_dataset, params[\"batch_size\"], params[\"patch_size\"], False)\n",
        "\n",
        "    # Recreate model\n",
        "    model = MLP(params[\"hidden_sizes\"] , params[\"activation\"], params[\"dropout_rate\"], params[\"batch_norm\"])\n",
        "\n",
        "    # Load saved weights\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X = X.float() / 255.0  # Same normalisation as training\n",
        "            outputs = model(X)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "            all_predicted.extend(preds.numpy())\n",
        "            all_labels.extend(y.numpy())\n",
        "\n",
        "    test_acc = (correct / total) * 100 # Percentage\n",
        "    return test_acc, all_predicted, all_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "jU7yOBhMAv3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/data/Q5_mlp.pt\"\n",
        "acc, preds, labels = test_mlp(model_path)\n",
        "\n",
        "print(acc)\n",
        "\n",
        "# make some visualisations maybe?"
      ],
      "metadata": {
        "id": "rl-OklYKG9xL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}